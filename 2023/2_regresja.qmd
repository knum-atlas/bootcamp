# Regresja {.unnumbered}

---
author: "KNUM ATLAS"
format: 
  html:
    toc: true
    embed-resources: true
    self-contained: true
    page-layout: full
date: "02-06-2023"
---

# Spotkanie 2 - Regresja

------------------------------------------------------------------------

## Regresja liniowa

### Czym jest regresja liniowa?

Przypomnienie z poprzedniego spotkania:

W zadaniu regresyjnym na podstawie zbioru cech obserwacji - w naszym przypadku danych samochodów

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = F, message = F)
```

```{r}
library(tidyverse)
library(kableExtra)
dane <- mtcars
head(dane) %>% 
    kbl() %>% 
  kable_styling() %>% 
  column_spec(c(3:12), background = "green")
  
```

Chcemy przewidywać zmienną typu ciągłego - w naszym przypadku spalanie

```{r}
head(dane) %>% 
    kbl() %>% 
  kable_styling() %>% 
  column_spec(2, background = "green")
```

Załóżmy że naszym celem jest ustalenie jak wygląda zależność pomiędzy liczbą koni mechanicznych a spalaniem

```{r}
ggplot(dane,aes(y = mpg, x = hp))+
  geom_point()
```

W celu rozwiązania tego zadania i znalezienia prostej która najlepiej odda tę zależność możemy posłużyć się regresją liniową.

W naszym wypadku wzór byłby następujący:

$$\text{mpg} = \beta_0 + \beta_1 \cdot \text{hp}$$

Gdzie:

-- $\beta_0$ jest wyrazem wolnym

-- $\beta_1$ jest współczynnikiem dla liczby koni mechanicznych

Co można uogólnić na dowolną liczbę cech których użyjemy do budowy modelu (oznaczone jako $x_i$)

$$y = \beta_0 + \beta_1x_1$$

### Jak wyznaczyć współczynniki?

Na poprzednim spotkaniu mówiliśmy o funkcjach straty oraz optymalizacji ich wartości w celu znalezienia najlepszego modelu - te same zasady dotyczą regresji liniowej!

Najpopularniejszą funkcją straty jest MSE, dla przypomnienia:

$$MSE = \frac{1}{n}\sum_{i=0}^{n}(y_i-\hat{y}_i)^2, $$ gdzie:

-- $n$ - liczba obserwacji <br> -- $y_i$ - i-ta obserwacja <br> -- $\hat{y}_i$ - predykcja dla i-tej obserwacji

Dla regresji z jedną zmienną będzie to:

$$MSE = \frac{1}{n}\sum_{i=0}^{n}(y_i-\beta_0 + \beta_1x_1)^2, $$

a dokładniej w naszym przypadku

$$MSE = \frac{1}{n}\sum_{i=0}^{n}(y_i-\beta_0 + \beta_1\cdot\text{hp})^2, $$

## Regresja w działaniu

```{r}
mod <- lm(dane$mpg~dane$hp,dane)
```

Linie regresji poprowadzone dla naszych obserwacji wyglądają następująco

```{r}
ggplot(dane,aes(y = mpg, x = hp))+
  geom_point()+
  #stat_smooth(method = "lm", se = F)+
  geom_abline(intercept = 30.09886, slope = -0.06823, color = "blue", size = 1)+
  geom_abline(intercept = 34, slope = -0.1,color = "red", size = 1)+
  geom_abline(intercept = 25, slope = -0.06,color = "purple", size = 1)
```

Z tych trzech natomiast niebieska jest regresją dla której oba współczniki są dobrane poprzez optymalizacje funkcji starty

Do określenia jakości naszego modelu możemy użyć metryki $R^2$ określanej jako kwadrat wyjaśnianej wriancji. Mowiąc prościej, ile procent zmienności zmiennej wynikowej jest uchwycone za pomocą wykorzystanych predyktorów.

Metryka ta przyjmuje wartości od $0$ co oznacza że nie ma żadnej zależności do $1$ co oznacza że zależność jest idelanie liniowa

Dla naszego modelu $R^2=0.59$ czyli za pomocą danych jedynie o koniach mechanicznych auta jesteśmy w stanie wyjaśnić około $60\%$ zmienności spalania.

## Regresja wielomianowa

```{r}
mod2 <- lm(mpg ~ poly(hp,2), dane)
#summary(mod2)
```

Można zauważyć, że na naszym przykładzie punkty układają się nie do końca w linii prostej dlatego można zastanowić się czy nie byłoby lepiej użyć do tego zadania jakiejś funkcji nieliniowej?

W tym celu możemy posłużyć się regresją wielomianową, jak sama nazwa wskazuje w tym przypadku zamiast linii prostej będziemy modelować zależność przy wykorzystaniu wielomianu.

Spróbujmy dodatkowo włączyć do modelu konie mechaniczne w drugiej potędze

Wzór na MSE będzie wyglądał podobnie jak dla modelu liniowego z tą różnicą, że dodajemy nasz predyktor w drugiej potędze z własnym współczynnikiem

$$MSE = \frac{1}{n}\sum_{i=0}^{n}(y_i-\beta_0 + \beta_1\cdot\text{hp} + \beta_2 \cdot \text{hp}^2)^2, $$

::: callout-warning
## UWAGA

Przy włączaniu predyktora w wyższej potędze, musi on być włączony również we wszystkich niższych potęgach.

Każdy z predyktorów ma inny współczynnik $\beta$ pomimo faktu że są tą samą cechą w różnych potęgach
:::

<br>

Nasza krzywa naniesiona na punkty wygląda następująco:

```{r}
ggplot(dane,aes(y = mpg, x = hp))+
  geom_point()+
  stat_smooth(method = "lm", formula = y~poly(x,degree = 2), se=F)
```

Model ten charakteruzyje się $R^2=0.76$ co czyni go lepszym od poprzedniego ponieważ wyjaśnia $16\%$ zmienności więcej.

Ale czy na pewno jest to aż tak dobry model?

<details>

<summary>Odpowiedzi</summary>

Do 200-250 koni mechanicznych zależność rzeczywiście jest lepiej odwzorowana niż w modelu liniowym, natomiast powyżej tej wartości spodziewamy się że spalanie powinno dalej rosnąć natomiast model powyżej 250 koni mechanicznych przewiduje, że spalanie będzie maleć.

Może do doprowadzić do sytuacji gdzie wprowadzona obserwacja spoza zakresu uczenia np. samochód z 500 koniami mechanicznymi z takiego modelu może uzyskać predykcje spalania podobną jak dla samochodu ze 100 koniami.

</ol>

</details>

Jak możemy w takim razie poprawć ten model?

Ułożenie punktów może nam sugerować, że lepsza tutaj będzie zależność hiperboliczna

```{r}
ggplot(dane,aes(y = mpg, x = hp))+
  geom_point()+
  stat_smooth(method = "lm", formula = y~I(1/x), se=F)
```

```{r}
mod2 <- lm(mpg ~ I(1/hp), dane)
#summary(mod2)
```

Model ten charakteruzyje się $R^2=0.74$ czyli tylko $2\%$ mniej od modelu opisanego wielomianem kwadratowym, natomiast w tym przypadku zależność ta będzie bliższa rzeczywistości.

## Regresja wieloraka

Przy wykorzytaniu jednej cechy ciężko będzie zamodelować zależności wystepujące w prawdziwym świecie :).

W celu uzyskania lepszego opisu zależności możemy wykorzystać regresję wieloraką, czyli regresję w której używamy wiele cech jednocześnie

Uogólniając wcześniej przedstawiony wzór na regresję liniową, regresję wieloraką zapisujemy następująco (gdzie $x_i$ to cechy)

$$y = \beta_0 + \beta_1x_1 + \beta_2x_2+\cdots+\beta_nx_n$$

Dla naszego zadania moglibyśmy zbudować następujący model

disp, hp, qsec, wt, drat

$$\text{mpg} = \beta_0 + \beta_1 \cdot \text{hp} + \beta_2 \cdot \text{disp} + \beta_3 \cdot \text{qsec} + \beta_4 \cdot \text{wt} + \beta_5 \cdot \text{drat}$$

```{r}
mod3 <- lm(mpg~hp+disp+qsec+wt+drat, dane)
#summary(mod3)
```

Ten model cechuje się $R^2 = 0.85$ czyli uzyskaliśmy w ten sposób $10\%$ więcej wyjaśnianej zmienności względem najlepszego modelu zbudowanego na tylko jednym predyktorze.

## Uwagi końcowe

-   Regresja wielomianowa i regresja wieloraka są również regresjami liniowymi.

-   W rzeczywistości lepszą miarą od $R^2$ jest jego skorygowana wersja oznaczana jako $R^2_{adj}$ ponieważ zwykłe $R^2$ będzie rosło pomimo dodawania do modelu nieistotnych predyktorów.

- W regresji można stosować regularyzację np. Ridge lub LASSO w celu uniknięcia nadmiernego dopasowania.

-   Regresja liniowa ma wiele założeń które trzeba spełnić aby móc poprawnie wnioskować na jej podstawie. W zadaniu predykcyjnym (tak jak w naszym przypadku) nie musimy się nimi przejmować ponieważ chcemy tylko aby model jak najlepiej przewidywał zmienną wynikową.

## Zadanie

Do dyspozycji macie dane ([link](https://drive.google.com/drive/u/3/folders/1DjpxNS2jocC_K4KK_slbXa-MLiPLp5oD)) dotyczące przewidywanej długości życia zebrane w 193 państwach na przestrzeni 15 lat.

Dane zawierają 18 kolumn z takimi informacjami jak:

-   Odsetek zgonów w różnym wieku

-   Szczepienia noworodków

-   Wskaźniki w kontekście państwa (np. GDP, wydatki na służbe zdrowia)

Zbudujcie model regresji liniowej przewidujący długość życia na podstawie dostępnych kolumn. Jakie wyszło RMSE? Czy błąd jest wysoki czy niski?

Dla zaawansowanych:

-   Uzupełnić braki danych.

-   Wykonać model w `tidymodels`.

-   Dodać regularyzację.
